{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Meta-labelling and the Triple Barrier Method Apple Stock",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iritwikdas/assignment_predict_now_ai/blob/main/Meta_labelling_and_the_Triple_Barrier_Method_Apple_Stock.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvB1LpsOkny7"
      },
      "source": [
        "# Flow of algorithm\n",
        "\n",
        "1. We will build a simple bollinger band strategy to identify long and short signals.\n",
        "\n",
        "2. We will then apply the Triple Barrier Method outline above to determine our triple barrier events. (Discussed in another python notebook in this repo)\n",
        "\n",
        "3. Then we will use forecasts from the primary model to generate meta-labels.\n",
        "\n",
        "4. Finally we will use a random forest algorithm on the meta-labels to filter the “Buy” and “Sell” signals and improve the overall precision of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TcmAAFaktFM"
      },
      "source": [
        "# Installing and Importing Requisite Packages (can skip this tab)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6s5ZC0eGEbW"
      },
      "source": [
        "# import standard libs\n",
        "from IPython.display import display\n",
        "from IPython.core.debugger import set_trace as bp\n",
        "from pathlib import PurePath, Path\n",
        "import sys\n",
        "import time\n",
        "import datetime as dt\n",
        "import multiprocessing as mp\n",
        "from datetime import datetime\n",
        "from collections import OrderedDict as od\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "\n",
        "# import python scientific stack\n",
        "import pandas as pd\n",
        "import pandas_datareader.data as web\n",
        "from pandas import Timestamp\n",
        "pd.set_option('display.max_rows', 100)\n",
        "from dask import dataframe as dd\n",
        "from dask.diagnostics import ProgressBar\n",
        "pbar = ProgressBar()\n",
        "pbar.register()\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "import statsmodels.api as sm\n",
        "from numba import jit\n",
        "import math\n",
        "import pymc3 as pm\n",
        "from theano import shared, theano as tt\n",
        "from sklearn.utils import resample\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "from itertools import cycle\n",
        "from scipy import interp\n",
        "\n",
        "# import visual tools\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import plotly\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "plt.style.use('seaborn-talk')\n",
        "plt.style.use('bmh')\n",
        "\n",
        "\n",
        "plt.rcParams['font.weight'] = 'medium'\n",
        "blue, green, red, purple, gold, teal = sns.color_palette('colorblind', 6)\n",
        "\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import missingno as msno\n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygEk0V7eYQji"
      },
      "source": [
        "# Defining some generic functions required (can skip this tab)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1sMKQFweckO"
      },
      "source": [
        "class MultiProcessingFunctions:\n",
        "\t\"\"\" This static functions in this class enable multi-processing\"\"\"\n",
        "\tdef __init__(self):\n",
        "\t\tpass\n",
        "\n",
        "\t@staticmethod\n",
        "\tdef lin_parts(num_atoms, num_threads):\n",
        "\t\t\"\"\" This function partitions a list of atoms in subsets (molecules) of equal size.\n",
        "\t\tAn atom is a set of indivisible set of tasks.\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\t# partition of atoms with a single loop\n",
        "\t\tparts = np.linspace(0, num_atoms, min(num_threads, num_atoms) + 1)\n",
        "\t\tparts = np.ceil(parts).astype(int)\n",
        "\t\treturn parts\n",
        "\n",
        "\t@staticmethod\n",
        "\tdef nested_parts(num_atoms, num_threads, upper_triangle=False):\n",
        "\t\t\"\"\" This function enables parallelization of nested loops.\n",
        "\t\t\"\"\"\n",
        "\t\t# partition of atoms with an inner loop\n",
        "\t\tparts = []\n",
        "\t\tnum_threads_ = min(num_threads, num_atoms)\n",
        "\n",
        "\t\tfor num in range(num_threads_):\n",
        "\t\t\tpart = 1 + 4 * (parts[-1] ** 2 + parts[-1] + num_atoms * (num_atoms + 1.) / num_threads_)\n",
        "\t\t\tpart = (-1 + part ** .5) / 2.\n",
        "\t\t\tparts.append(part)\n",
        "\n",
        "\t\tparts = np.round(parts).astype(int)\n",
        "\n",
        "\t\tif upper_triangle:  # the first rows are heaviest\n",
        "\t\t\tparts = np.cumsum(np.diff(parts)[::-1])\n",
        "\t\t\tparts = np.append(np.array([0]), parts)\n",
        "\t\treturn parts\n",
        "\n",
        "\t@staticmethod\n",
        "\tdef mp_pandas_obj(func, pd_obj, num_threads=24, mp_batches=1, lin_mols=True, **kargs):\n",
        "\t\t\"\"\"\t\n",
        "\t\t:param func: (string) function to be parallelized\n",
        "\t\t:param pd_obj: (vector) Element 0, is name of argument used to pass the molecule;\n",
        "\t\t\t\t\t\tElement 1, is the list of atoms to be grouped into a molecule\n",
        "\t\t:param num_threads: (int) number of threads\n",
        "\t\t:param mp_batches: (int) number of batches\n",
        "\t\t:param lin_mols: (bool) Tells if the method should use linear or nested partitioning\n",
        "\t\t:param kargs: (var args)\n",
        "\t\t:return: (data frame) of results\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tif lin_mols:\n",
        "\t\t\tparts = MultiProcessingFunctions.lin_parts(len(pd_obj[1]), num_threads * mp_batches)\n",
        "\t\telse:\n",
        "\t\t\tparts = MultiProcessingFunctions.nested_parts(len(pd_obj[1]), num_threads * mp_batches)\n",
        "\n",
        "\t\tjobs = []\n",
        "\t\tfor i in range(1, len(parts)):\n",
        "\t\t\tjob = {pd_obj[0]: pd_obj[1][parts[i - 1]:parts[i]], 'func': func}\n",
        "\t\t\tjob.update(kargs)\n",
        "\t\t\tjobs.append(job)\n",
        "\n",
        "\t\tif num_threads == 1:\n",
        "\t\t\tout = MultiProcessingFunctions.process_jobs_(jobs)\n",
        "\t\telse:\n",
        "\t\t\tout = MultiProcessingFunctions.process_jobs(jobs, num_threads=num_threads)\n",
        "\n",
        "\t\tif isinstance(out[0], pd.DataFrame):\n",
        "\t\t\tdf0 = pd.DataFrame()\n",
        "\t\telif isinstance(out[0], pd.Series):\n",
        "\t\t\tdf0 = pd.Series()\n",
        "\t\telse:\n",
        "\t\t\treturn out\n",
        "\n",
        "\t\tfor i in out:\n",
        "\t\t\tdf0 = df0.append(i)\n",
        "\n",
        "\t\tdf0 = df0.sort_index()\n",
        "\t\treturn df0\n",
        "\n",
        "\t@staticmethod\n",
        "\tdef process_jobs_(jobs):\n",
        "\t\t\"\"\" Run jobs sequentially, for debugging \"\"\"\n",
        "\t\tout = []\n",
        "\t\tfor job in jobs:\n",
        "\t\t\tout_ = MultiProcessingFunctions.expand_call(job)\n",
        "\t\t\tout.append(out_)\n",
        "\t\treturn out\n",
        "\n",
        "\t@staticmethod\n",
        "\tdef expand_call(kargs):\n",
        "\t\t\"\"\" Expand the arguments of a callback function, kargs['func'] \"\"\"\n",
        "\t\tfunc = kargs['func']\n",
        "\t\tdel kargs['func']\n",
        "\t\tout = func(**kargs)\n",
        "\t\treturn out\n",
        "\n",
        "\t@staticmethod\n",
        "\tdef report_progress(job_num, num_jobs, time0, task):\n",
        "\t\t# Report progress as asynch jobs are completed\n",
        "\n",
        "\t\tmsg = [float(job_num) / num_jobs, (time.time() - time0)/60.]\n",
        "\t\tmsg.append(msg[1] * (1/msg[0] - 1))\n",
        "\t\ttime_stamp = str(dt.datetime.fromtimestamp(time.time()))\n",
        "\n",
        "\t\tmsg = time_stamp + ' ' + str(round(msg[0]*100, 2)) + '% '+task+' done after ' + \\\n",
        "\t\t\tstr(round(msg[1], 2)) + ' minutes. Remaining ' + str(round(msg[2], 2)) + ' minutes.'\n",
        "\n",
        "\t\tif job_num < num_jobs:\n",
        "\t\t\tsys.stderr.write(msg+'\\r')\n",
        "\t\telse:\n",
        "\t\t\tsys.stderr.write(msg+'\\n')\n",
        "\n",
        "\t\treturn\n",
        "\n",
        "\t@staticmethod\n",
        "\tdef process_jobs(jobs, task=None, num_threads=24):\n",
        "\t\t\"\"\" Run in parallel. jobs must contain a 'func' callback, for expand_call\"\"\"\n",
        "\n",
        "\t\tif task is None:\n",
        "\t\t\ttask = jobs[0]['func'].__name__\n",
        "\n",
        "\t\tpool = mp.Pool(processes=num_threads)\n",
        "\t\t# outputs, out, time0 = pool.imap_unordered(MultiProcessingFunctions.expand_call,jobs),[],time.time()\n",
        "\t\toutputs = pool.imap_unordered(MultiProcessingFunctions.expand_call, jobs)\n",
        "\t\tout = []\n",
        "\t\ttime0 = time.time()\n",
        "\n",
        "\t\t# Process asyn output, report progress\n",
        "\t\tfor i, out_ in enumerate(outputs, 1):\n",
        "\t\t\tout.append(out_)\n",
        "\t\t\tMultiProcessingFunctions.report_progress(i, len(jobs), time0, task)\n",
        "\n",
        "\t\tpool.close()\n",
        "\t\tpool.join()  # this is needed to prevent memory leaks\n",
        "\t\treturn out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BLsJSBvmSuF"
      },
      "source": [
        "# Daily Volatility\n",
        "\n",
        "Now we will calculate Daily Volatility in order to create dynamic thresholds.\n",
        "\n",
        "**Parameters:**\n",
        "* close: (data frame) Closing prices\n",
        "* lookback: (int) lookback period to compute volatility\n",
        "\n",
        "**Return/\"Output of the function\": **\n",
        "\n",
        "* (series) of daily volatility value\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9oyhWgHJbpa"
      },
      "source": [
        "def get_daily_vol(close, lookback=100):\n",
        "    \n",
        "    print('Calculating daily volatility for dynamic thresholds')\n",
        "    \n",
        "    df0 = close.index.searchsorted(close.index - pd.Timedelta(days=1))\n",
        "    df0 = df0[df0 > 0]\n",
        "    df0 = (pd.Series(close.index[df0 - 1], index=close.index[close.shape[0] - df0.shape[0]:]))\n",
        "        \n",
        "    df0 = close.loc[df0.index] / close.loc[df0.values].values - 1  # daily returns\n",
        "    df0 = df0.ewm(span=lookback).std()\n",
        "    return df0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3gOTwe3BMGr"
      },
      "source": [
        "# Creating Events/Triggers\n",
        "\n",
        "We need to create our own trigger to signal the beginning of each new window. For this, we define a new function called **get_t_events**.\n",
        "\n",
        "We will use **Symmetric CUSUM Filter** as a method for *detecting a shift in the mean value of a measured quantity away from a target value.*\n",
        "\n",
        "The filter is set up to utilize the daily volatility we derived above to determine the threshold for labelling an event.\n",
        "\n",
        "> *For example, if the average volatility is 3%, once the volatility exceeds 3% net change since our last event, we would generate a new event and reset our filter.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQ1QQgrtxHo7"
      },
      "source": [
        "# CUSUM Filter\n",
        "\n",
        "Symmetric CUSUM Filter as a method for “detecting a shift in the mean value of a measured quantity away from a target value.” (de Prado, 38). The filter is set up to use the daily volatility we derived above to determine the threshold for labelling an event. For example, if our average volatility is 2%, once we exceed a 2% net change since our last event, we would generate a new event and reset our filter.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBxg-p1kW3Kn"
      },
      "source": [
        "def get_t_events(raw_price, threshold):\n",
        "    \n",
        "    print('Applying Symmetric CUSUM filter.')\n",
        "\n",
        "    t_events = []\n",
        "    s_pos = 0\n",
        "    s_neg = 0\n",
        "\n",
        "    # log returns\n",
        "    diff = np.log(raw_price).diff().dropna()\n",
        "\n",
        "    # Get event time stamps for the entire series\n",
        "    for i in tqdm(diff.index[1:]):\n",
        "        pos = float(s_pos + diff.loc[i])\n",
        "        neg = float(s_neg + diff.loc[i])\n",
        "        s_pos = max(0.0, pos)\n",
        "        s_neg = min(0.0, neg)\n",
        "\n",
        "        if s_neg < -threshold:\n",
        "            s_neg = 0\n",
        "            t_events.append(i)\n",
        "\n",
        "        elif s_pos > threshold:\n",
        "            s_pos = 0\n",
        "            t_events.append(i)\n",
        "\n",
        "    event_timestamps = pd.DatetimeIndex(t_events)\n",
        "    return event_timestamps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEjWcYnKuSxg"
      },
      "source": [
        "# Vertical Barriers\n",
        "\n",
        "After obtaining the list of event timestamps, we define the function **add_vertical_barrier** to output ***a series of all the timestamps when the vertical barrier is reached***. \n",
        "\n",
        "Here we create an argument num_days to adjust the time duration (in days) we want the barrier to stay active. In regards to this assignment it is 5 days."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAtItBi7HxbI"
      },
      "source": [
        "def add_vertical_barrier(t_events, close, num_days=5):\n",
        "    t1 = close.index.searchsorted(t_events + pd.Timedelta(days=num_days))\n",
        "    t1 = t1[t1 < close.shape[0]]\n",
        "    t1 = pd.Series(close.index[t1], index=t_events[:t1.shape[0]])  # NaNs at end\n",
        "    return t1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B63LBDPVvX2e"
      },
      "source": [
        "# Time of First Touch\n",
        "\n",
        "The function apply_pt_sl_on_t1 applies the triple barrier labels and outputs a dataframe with the timestamps at which each barrier was touched.\n",
        "We want to use the apply_pt_sl_on_t1 within the function get_events to incorporate the results of the previous functions. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9hqLRGbH1Og"
      },
      "source": [
        "def apply_pt_sl_on_t1(close, events, pt_sl, molecule):\n",
        "    \"\"\"\n",
        "    :param events: (series) of indices that signify \"events\" \n",
        "    :param pt_sl: (array) element 0, indicates the profit taking level; \n",
        "                          element 1 is stop loss level\n",
        "    :param molecule: (an array) a set of datetime index values for processing\n",
        "    \"\"\"\n",
        "    \n",
        "    # apply stop loss/profit taking, if it takes place before t1 (end of event)\n",
        "    events_ = events.loc[molecule]\n",
        "    out = events_[['t1']].copy(deep=True)\n",
        "    if pt_sl[0] > 0:\n",
        "        pt = pt_sl[0] * events_['trgt']\n",
        "    else:\n",
        "        pt = pd.Series(index=events.index)  # NaNs\n",
        "\n",
        "    if pt_sl[1] > 0:\n",
        "        sl = -pt_sl[1] * events_['trgt']\n",
        "    else:\n",
        "        sl = pd.Series(index=events.index)  # NaNs\n",
        "\n",
        "    for loc, t1 in events_['t1'].fillna(close.index[-1]).iteritems():\n",
        "        df0 = close[loc:t1]  # path prices\n",
        "        df0 = (df0 / close[loc] - 1) * events_.at[loc, 'side']  # path returns\n",
        "        out.loc[loc, 'sl'] = df0[df0 < sl[loc]].index.min()  # earliest stop loss\n",
        "        out.loc[loc, 'pt'] = df0[df0 > pt[loc]].index.min()  # earliest profit taking\n",
        "\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "am_9pVnEyzTO"
      },
      "source": [
        "get_events also allows us the ability to incorporate the side of a bet (decided by a separate primary model) in order to effectively use profit-take and stop-loss limits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdHT9h9l-BkZ"
      },
      "source": [
        "\n",
        "```\n",
        "t_events: (series) of t_events. These are timestamps that will seed every triple barrier.\n",
        "\n",
        "pt_sl: (2 element array) element 0, indicates the profit taking level;element 1 is stop loss level. \n",
        "A non-negative float that sets the width of the two barriers. \n",
        "A 0 value means that the respective horizontal barrier will be disabled.\n",
        "\n",
        "target: (series) of values that are used (in conjunction with pt_sl)to determine the width of the barrier.\n",
        "\n",
        "min_ret: (float) The minimum target return required for running a triple barrier search.\n",
        "\n",
        "num_threads: (int) The number of threads concurrently used by the function.\n",
        "\n",
        "vertical_barrier_times: (series) A pandas series with the timestamps of the vertical barriers.\n",
        "\n",
        "side: (series) Side of the bet (long/short) as decided by the primary model\n",
        "\n",
        "return: (data frame) of events\n",
        "            -events.index is event's starttime\n",
        "            -events['t1'] is event's endtime\n",
        "            -events['trgt'] is event's target\n",
        "            -events['side'] (optional) implies the algo's position side\n",
        "\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfSf2AXdH4uL"
      },
      "source": [
        "def get_events(close, t_events, pt_sl, target, min_ret, num_threads, \n",
        "              vertical_barrier_times=False, side=None):\n",
        "    \n",
        "    # 1) Get target\n",
        "    target = target.loc[target.index.intersection(t_events)]\n",
        "    target = target[target > min_ret]  # min_ret\n",
        "\n",
        "    # 2) Get vertical barrier (max holding period)\n",
        "    if vertical_barrier_times is False:\n",
        "        vertical_barrier_times = pd.Series(pd.NaT, index=t_events)\n",
        "\n",
        "    # 3) Form events object, apply stop loss on vertical barrier\n",
        "    if side is None:\n",
        "        side_ = pd.Series(1., index=target.index)\n",
        "        pt_sl_ = [pt_sl[0], pt_sl[0]]\n",
        "    else:\n",
        "        side_ = side.loc[target.index]\n",
        "        pt_sl_ = pt_sl[:2]\n",
        "\n",
        "    events = pd.concat({'t1': vertical_barrier_times, 'trgt': target, 'side': side_},\n",
        "                        axis=1)\n",
        "    events = events.dropna(subset=['trgt'])\n",
        "\n",
        "    # Apply Triple Barrier\n",
        "    df0 = MultiProcessingFunctions.mp_pandas_obj(func=apply_pt_sl_on_t1,\n",
        "                                                 pd_obj=('molecule', events.index),\n",
        "                                                 num_threads=num_threads,\n",
        "                                                 close=close,\n",
        "                                                 events=events,\n",
        "                                                 pt_sl=pt_sl_)\n",
        "\n",
        "    events['t1'] = df0.dropna(how='all').min(axis=1)  # pd.min ignores nan\n",
        "\n",
        "    if side is None:\n",
        "        events = events.drop('side', axis=1)\n",
        "\n",
        "    return events"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uK27u02p-xPx"
      },
      "source": [
        "# Checking if Top/Bottom/Vertical Barrier is touched\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwgOeAKfmomo"
      },
      "source": [
        "def barrier_touched(out_df):\n",
        "    store = []\n",
        "    for i in np.arange(len(out_df)):\n",
        "        date_time = out_df.index[i]\n",
        "        ret = out_df.loc[date_time, 'ret']\n",
        "        target = out_df.loc[date_time, 'trgt']\n",
        "\n",
        "        if ret > 0.0 and ret > target:\n",
        "            # Top barrier reached\n",
        "            store.append(1)\n",
        "        elif ret < 0.0 and ret < -target:\n",
        "            # Bottom barrier reached\n",
        "            store.append(-1)\n",
        "        else:\n",
        "            # Vertical barrier reached\n",
        "            store.append(0)\n",
        "\n",
        "    out_df['bin'] = store\n",
        "\n",
        "    return out_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdCVkZZo6OBS"
      },
      "source": [
        "# Metalabelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnvvEoey60RZ"
      },
      "source": [
        "We’ll then use a new function ***get_bins*** to meta-label each event with a 0 or a 1 based on whether or not the primary model achieved the correct prediction.\n",
        "\n",
        "1. You can think about this logic like this:\n",
        "If our primary model (side) indicated a position and our return at the end of the triple barrier event was positive, we would label that “bin” as a 1 (true positive).\n",
        "\n",
        "2. If our primary model (side) indicated a position and our return at the end of the triple barrier event was negative, we would label that “bin” as a 0 (false positive)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe5LCM7C5m8d"
      },
      "source": [
        "\n",
        "     triple_barrier_events: (data frame)\n",
        "                -events.index is event's starttime\n",
        "                -events['t1'] is event's endtime\n",
        "                -events['trgt'] is event's target\n",
        "                -events['side'] (optional) implies the algo's position side\n",
        "                \n",
        "                Case 1: ('side' not in events): bin in (-1,1) <-label by price action\n",
        "                Case 2: ('side' in events): bin in (0,1) <-label by pnl (meta-labeling)\n",
        "     \n",
        "      return: (data frame) of meta-labeled events\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G93v7IztYsLi"
      },
      "source": [
        "def get_bins(triple_barrier_events, close):\n",
        "    \n",
        "\n",
        "    # 1) Align prices with their respective events\n",
        "    events_ = triple_barrier_events.dropna(subset=['t1'])\n",
        "    prices = events_.index.union(events_['t1'].values)\n",
        "    prices = prices.drop_duplicates()\n",
        "    prices = close.reindex(prices, method='bfill')\n",
        "    \n",
        "    # 2) Create out DataFrame\n",
        "    out_df = pd.DataFrame(index=events_.index)\n",
        "    # Need to take the log returns, else your results will be skewed for short positions\n",
        "    out_df['ret'] = np.log(prices.loc[events_['t1'].values].values) - np.log(prices.loc[events_.index])\n",
        "    out_df['trgt'] = events_['trgt']\n",
        "\n",
        "    # Meta labeling: Events that were correct will have pos returns\n",
        "    if 'side' in events_:\n",
        "        out_df['ret'] = out_df['ret'] * events_['side']  # meta-labeling\n",
        "\n",
        "    # Added code: label 0 when vertical barrier reached\n",
        "    out_df = barrier_touched(out_df)\n",
        "\n",
        "    # Meta labeling: label incorrect events with a 0\n",
        "    if 'side' in events_:\n",
        "        out_df.loc[out_df['ret'] <= 0, 'bin'] = 0\n",
        "    \n",
        "    # Transform the log returns back to normal returns.\n",
        "    out_df['ret'] = np.exp(out_df['ret']) - 1\n",
        "    \n",
        "    # Add the side to the output. This is useful for when a meta label model must be fit\n",
        "    tb_cols = triple_barrier_events.columns\n",
        "    if 'side' in tb_cols:\n",
        "        out_df['side'] = triple_barrier_events['side']\n",
        "        \n",
        "    out_df\n",
        "\n",
        "    return out_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hu4YussRQ7XJ"
      },
      "source": [
        "def bbands(close_prices, window, no_of_stdev):\n",
        "    \n",
        "    rolling_mean = close_prices.ewm(span=window).mean()\n",
        "    rolling_std = close_prices.ewm(span=window).std()\n",
        "\n",
        "    upper_band = rolling_mean + (rolling_std * no_of_stdev)\n",
        "    lower_band = rolling_mean - (rolling_std * no_of_stdev)\n",
        "\n",
        "    return rolling_mean, upper_band, lower_band"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lB-CuBIffyGf"
      },
      "source": [
        "# Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrExdiM6YZU9"
      },
      "source": [
        "Import and Format Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsgvIUx7jW82"
      },
      "source": [
        "%%capture\n",
        "pip install yfinance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYZBCe69jfJH"
      },
      "source": [
        "import yfinance as yf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTJ6rY1EjStR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47c60d35-d3c0-4e53-ec06-07cf35468792"
      },
      "source": [
        "def get_data(symbols, begin_date=None,end_date=None):\n",
        "    df = yf.download('AAPL', start = begin_date,\n",
        "                     auto_adjust=True,#only download adjusted data\n",
        "                     end= end_date) \n",
        "    #my convention: always lowercase\n",
        "    df.columns = ['open','high','low',\n",
        "                  'close','volume'] \n",
        "    \n",
        "    return df\n",
        "\n",
        "Apple_stock = get_data('AAPL', '1999-12-31', '2021-10-20')   \n",
        "data = Apple_stock"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kddXsl0v6ZIj"
      },
      "source": [
        "# Primary Model\n",
        "# **Bollinger Bands**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xoxuSywYc0r"
      },
      "source": [
        "Create Primary Bollinger Band Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQUBffSDY1YF",
        "outputId": "50d34ea7-057e-4f62-90b4-22e1bfcd4947"
      },
      "source": [
        "# compute bands\n",
        "window = 20\n",
        "data['avg'], data['upper'], data['lower'] = bbands(data['close'],window, no_of_stdev=1.5)\n",
        "\n",
        "# compute sides\n",
        "data['side'] = np.nan\n",
        "long_signals = (data['close'] <= data['lower'])\n",
        "short_signals = (data['close'] >= data['upper'])\n",
        "data.loc[long_signals, 'side'] = 1\n",
        "data.loc[short_signals, 'side'] = -1\n",
        "\n",
        "print(data.side.value_counts())\n",
        "\n",
        "# Remove Look ahead biase by lagging the signal\n",
        "data['side'] = data['side'].shift(1)\n",
        "\n",
        "# Drop the NaN values from our data set\n",
        "data.dropna(axis=0, how='any', inplace=True)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-1.0    243\n",
            " 1.0    128\n",
            "Name: side, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4nxfl1mY5ww"
      },
      "source": [
        "## Implementing Triple Barriers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "o6DgofNjYzky",
        "outputId": "e901a662-7bcb-4f97-d5a3-38d9bb37bc6b"
      },
      "source": [
        "close = data['close']\n",
        "\n",
        "# determining daily volatility using the last 50 days\n",
        "daily_vol = get_daily_vol(close=close, lookback=50)\n",
        "\n",
        "# creating our event triggers using the CUSUM filter \n",
        "cusum_events = get_t_events(close, threshold=daily_vol.mean()*0.1)\n",
        "\n",
        "# adding vertical barriers with a half day expiration window\n",
        "vertical_barriers = add_vertical_barrier(t_events=cusum_events,\n",
        "                                         close=close, num_days=5)\n",
        "\n",
        "# determining timestamps of first touch   \n",
        "\n",
        "pt_sl = [2, 2] # setting profit-take and stop-loss at 1% and 2%\n",
        "min_ret = 0.0005 # setting a minimum return of 0.05%\n",
        "\n",
        "triple_barrier_events = get_events(close=close,\n",
        "                                  t_events=cusum_events,\n",
        "                                  pt_sl=pt_sl,\n",
        "                                  target=daily_vol,\n",
        "                                  min_ret=min_ret,\n",
        "                                  num_threads=2,\n",
        "                                  vertical_barrier_times=vertical_barriers,\n",
        "                                  side=data['side'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating daily volatility for dynamic thresholds\n",
            "Applying Symmetric CUSUM filter.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 369/369 [00:00<00:00, 15119.26it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-115-d4aae38332f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m                                   \u001b[0mnum_threads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                                   \u001b[0mvertical_barrier_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvertical_barriers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                                   side=data['side'])\n\u001b[0m",
            "\u001b[0;32m<ipython-input-107-8aa5ff0d7f58>\u001b[0m in \u001b[0;36mget_events\u001b[0;34m(close, t_events, pt_sl, target, min_ret, num_threads, vertical_barrier_times, side)\u001b[0m\n\u001b[1;32m     28\u001b[0m                                                  \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                                                  \u001b[0mevents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                                                  pt_sl=pt_sl_)\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mevents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m't1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pd.min ignores nan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-102-98cf899eb45c>\u001b[0m in \u001b[0;36mmp_pandas_obj\u001b[0;34m(func, pd_obj, num_threads, mp_batches, lin_mols, **kargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlin_mols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiProcessingFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin_parts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd_obj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_threads\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmp_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiProcessingFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnested_parts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd_obj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_threads\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmp_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: type object 'MultiProcessingFunctions' has no attribute 'lin_parts'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2IQeQ_kQ1kh"
      },
      "source": [
        "Add Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWMSJt9VZBJa"
      },
      "source": [
        "labels = get_bins(triple_barrier_events, data['close'])\n",
        "labels.side.value_counts()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLkUok2--k98"
      },
      "source": [
        "labels.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrxoOuyTQ3V8"
      },
      "source": [
        "# Evaluating Primary Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3VDEJJMZV1t"
      },
      "source": [
        "# creating dataframe of only bin labels\n",
        "primary_forecast = pd.DataFrame(labels['bin'])\n",
        "\n",
        "# setting predicted column to 1 \n",
        "primary_forecast['pred'] = 1\n",
        "primary_forecast.columns = ['actual', 'pred']\n",
        "\n",
        "# Performance Metrics\n",
        "actual = primary_forecast['actual']\n",
        "pred = primary_forecast['pred']\n",
        "print(classification_report(y_true=actual, y_pred=pred))\n",
        "\n",
        "print(\"Confusion Matrix\")\n",
        "print(confusion_matrix(actual, pred))\n",
        "\n",
        "print('')\n",
        "print(\"Accuracy\")\n",
        "print(accuracy_score(actual, pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkrqcuNz1_Kk"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Upon evaluation of primary model we see that the accurace is 0.146 ie 14.6%.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_npquZde9-TZ"
      },
      "source": [
        "primary_forecast = pd.DataFrame(labels['bin'])\n",
        "primary_forecast"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrWSVElfZh_P"
      },
      "source": [
        "# Creating Secondary Model\n",
        "# Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6y3nt6D8RN6"
      },
      "source": [
        "# Get features at event dates\n",
        "X = data.loc[labels.index, :]\n",
        "y = labels['bin']\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "# Setting random forest parameters\n",
        "n_estimator = 100\n",
        "depth = 2\n",
        "RANDOM_STATE = 0\n",
        "\n",
        "rf = RandomForestClassifier(max_depth=depth, n_estimators=n_estimator,\n",
        "                            criterion='entropy', class_weight='balanced_subsample',\n",
        "                            random_state=RANDOM_STATE)\n",
        "\n",
        "# Fitting our model\n",
        "rf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_c0rScbEQ8j5"
      },
      "source": [
        "# Evaluating Secondary Model ***Training*** Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOXJqV2JZspp"
      },
      "source": [
        "# Performance Metrics\n",
        "y_pred_rf = rf.predict_proba(X_train)[:, 1]\n",
        "y_pred = rf.predict(X_train)\n",
        "fpr_rf, tpr_rf, _ = roc_curve(y_train, y_pred_rf)\n",
        "print(classification_report(y_train, y_pred))\n",
        "\n",
        "print(\"Confusion Matrix\")\n",
        "print(confusion_matrix(y_train, y_pred))\n",
        "\n",
        "print('')\n",
        "print(\"Accuracy\")\n",
        "print(accuracy_score(y_train, y_pred))\n",
        "\n",
        "plt.figure(1)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.plot(fpr_rf, tpr_rf, label='RF')\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8Ax6AHXQ_51"
      },
      "source": [
        "# Evaluating Secondary Model ***Testing*** Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9UbXoGvZvFL"
      },
      "source": [
        "# Performance Metrics\n",
        "y_pred_rf = rf.predict_proba(X_test)[:, 1]\n",
        "y_pred = rf.predict(X_test)\n",
        "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_rf)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Confusion Matrix\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print('')\n",
        "print(\"Accuracy\")\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "\n",
        "plt.figure(1)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.plot(fpr_rf, tpr_rf, label='RF')\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2t2ojD770VfY"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Upon using metalabelling our accuracy now has jumped to 0.81 or 81% from 14.6% (initially in primary model)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJTPqZCP9KYO"
      },
      "source": [
        "# *The speciality of using meta-labelling is that we can build an initial model that will maximizes recall, even if it have many false positives, and then incorporate a second model that filters out the false positives to enhance our precision score.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wP40jEFvytF8"
      },
      "source": [
        "labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7YVELWz8LSu"
      },
      "source": [
        "labels.to_csv('labels.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}